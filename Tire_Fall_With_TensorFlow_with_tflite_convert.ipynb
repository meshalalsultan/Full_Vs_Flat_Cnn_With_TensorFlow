{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Tire Fall With TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/meshalalsultan/Full_Vs_Flat_Cnn_With_TensorFlow/blob/main/Tire_Fall_With_TensorFlow.ipynb",
      "authorship_tag": "ABX9TyOSA6IuixnqIqzh9LCceELd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meshalalsultan/Full_Vs_Flat_Cnn_With_TensorFlow/blob/main/Tire_Fall_With_TensorFlow_with_tflite_convert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlelIbc8hcdv"
      },
      "source": [
        "# Full vs Flat Tire Images \n",
        "\n",
        "## Classify images of properly Filled and Underfilled Tires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pFix69ohvG4"
      },
      "source": [
        "The Contest and information about the data will be found here :\n",
        "\n",
        "https://github.com/meshalalsultan/Full_Vs_Flat_Cnn_With_TensorFlow/blob/main/README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNYL2YXXI83S"
      },
      "source": [
        "I get the data throw kaggle , and split them in respect to eatch class in floder name tire_dataset.zip , witch can found in https://github.com/meshalalsultan/Full_Vs_Flat_Cnn_With_TensorFlow/blob/main/tire_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiO8NBcteMil"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "#/content/drive/MyDrive/tire_dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlHTOW--JT93"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the file\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/tire_dataset.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkF53QN5Nz4h"
      },
      "source": [
        "ls tire_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2a02PFuN6cl"
      },
      "source": [
        "ls tire_dataset/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x71ZXCl4OBWt"
      },
      "source": [
        "ls tire_dataset/train/full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hXk5uMKOLWn"
      },
      "source": [
        "import os\n",
        "\n",
        "# Walk through pizza_steak directory and list number of files\n",
        "for dirpath, dirnames, filenames in os.walk(\"tire_dataset\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORQ8a33COrdm"
      },
      "source": [
        "## How many image in eatch class file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B28f8F-sOcp3"
      },
      "source": [
        "# Another way to find out how many images are in a file\n",
        "num_full_images_train = len(os.listdir(\"tire_dataset/train/full\"))\n",
        "\n",
        "num_full_images_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0LaQOWYO0OJ"
      },
      "source": [
        "num_flat_images_train = len(os.listdir(\"tire_dataset/train/flat\"))\n",
        "\n",
        "num_flat_images_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVjNkNaYOzxJ"
      },
      "source": [
        "num_no_tire_images_train = len(os.listdir(\"tire_dataset/train/no_tire\"))\n",
        "\n",
        "num_no_tire_images_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NzI_G-VPKB2"
      },
      "source": [
        "## Get the class name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J9P-mhSPJbs"
      },
      "source": [
        "# Get the class names (programmatically, this is much more helpful with a longer list of classes)\n",
        "import pathlib\n",
        "import numpy as np\n",
        "data_dir = pathlib.Path(\"tire_dataset/train/\") # turn our training path into a Python path\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uENOlh8PZGU"
      },
      "source": [
        "So now i have 261 image in train for eatch class , and 40 test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QejA81PoWO"
      },
      "source": [
        "## View the image and shape function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOHrIyzFPucQ"
      },
      "source": [
        "# View an image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "\n",
        "def view_random_image(target_dir, target_class):\n",
        "  # Setup target directory (we'll view images from here)\n",
        "  target_folder = target_dir+target_class\n",
        "\n",
        "  # Get a random image path\n",
        "  random_image = random.sample(os.listdir(target_folder), 1)\n",
        "\n",
        "  # Read in the image and plot it using matplotlib\n",
        "  img = mpimg.imread(target_folder + \"/\" + random_image[0])\n",
        "  plt.imshow(img)\n",
        "  plt.title(target_class)\n",
        "  plt.axis(\"off\");\n",
        "\n",
        "  print(f\"Image shape: {img.shape}\") # show the shape of the image\n",
        "\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPwC-SRrPw6t"
      },
      "source": [
        "# View a random image from the training dataset\n",
        "img = view_random_image(target_dir=\"tire_dataset/train/\",\n",
        "                        target_class=\"full\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QUPNs9IoIgf"
      },
      "source": [
        "# Vesualize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgc4j8dDmZX1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8TsNkWoSP1"
      },
      "source": [
        "full_img = cv2.imread('/content/tire_dataset/train/full/00000.jpg')\n",
        "full_img = cv2.cvtColor(full_img,cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSlzkUsouWA"
      },
      "source": [
        "#Check the type\n",
        "type(full_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG7QZx38o--3"
      },
      "source": [
        "full_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfleQlFtpC9Y"
      },
      "source": [
        "plt.imshow(full_img);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2zJ3oAQpPyQ"
      },
      "source": [
        "Check the flat image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQWTPbkrpJEA"
      },
      "source": [
        "flat_img = cv2.imread('/content/tire_dataset/train/flat/00004.jpg')\n",
        "flat_img = cv2.cvtColor(flat_img,cv2.COLOR_BGRA2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUn4ckUip0IR"
      },
      "source": [
        "#Check the type\n",
        "type(full_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4JRLBv9qBZ8"
      },
      "source": [
        "full_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-qq06rUqEj8"
      },
      "source": [
        "plt.imshow(full_img);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2tzzzL6rF-R"
      },
      "source": [
        "#Preprosess the image and generate more data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjkCEzvkQe1T"
      },
      "source": [
        "scale the image from 0 , 1\n",
        "by deviding /255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfA2a9q5QpE2"
      },
      "source": [
        "#Test the scale\n",
        "img/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_qjqtkkrNMK"
      },
      "source": [
        "## Zoom , Scale , rotate ,flip , rescale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXD8MjAqPO1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUTwNxwOrwBa"
      },
      "source": [
        "img_gen = ImageDataGenerator( rotation_range=30 , #rotate the image by 30%\n",
        "                             width_shift_range=0.1, #shift the img width by max 10%\n",
        "                             height_shift_range=0.1, #shift the img heigh bay max 10%\n",
        "                             rescale=1/255, #rescale img by normalize it\n",
        "                             shear_range=0.2, #cutting away part of the img as buffer \n",
        "                             zoom_range=0.2, #zoom in be 20%\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5mn8wIdtNpJ"
      },
      "source": [
        "plt.imshow(img_gen.random_transform(full_img));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETHnLBsjtoOP"
      },
      "source": [
        "plt.imshow(img_gen.random_transform(full_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhJ0YlF9tzxA"
      },
      "source": [
        "plt.imshow(img_gen.random_transform(flat_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDU267Lmt_qp"
      },
      "source": [
        "plt.imshow(img_gen.random_transform(flat_img));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrf2c6NZQqpf"
      },
      "source": [
        "# Building the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quUjbd-tQu-1"
      },
      "source": [
        "## Model_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0c8q3nXUFZC"
      },
      "source": [
        "build the model without add target_size=(240,240) in flow_from function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsjCDjc_QuNW"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Preprosses the data ( get them all in scale)\n",
        "train_datagen = ImageDataGenerator( rotation_range=30 , #rotate the image by 30%\n",
        "                             width_shift_range=0.1, #shift the img width by max 10%\n",
        "                             height_shift_range=0.1, #shift the img heigh bay max 10%\n",
        "                             rescale=1/255, #rescale img by normalize it\n",
        "                             shear_range=0.2, #cutting away part of the img as buffer \n",
        "                             zoom_range=0.2, #zoom in be 20%\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "valied_datagen = ImageDataGenerator( rotation_range=30 , #rotate the image by 30%\n",
        "                             width_shift_range=0.1, #shift the img width by max 10%\n",
        "                             height_shift_range=0.1, #shift the img heigh bay max 10%\n",
        "                             rescale=1/255, #rescale img by normalize it\n",
        "                             shear_range=0.2, #cutting away part of the img as buffer \n",
        "                             zoom_range=0.2, #zoom in be 20%\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "#Setup the train and test directories\n",
        "train_dir = 'tire_dataset/train/'\n",
        "test_dir = 'tire_dataset/test/'\n",
        "\n",
        "#Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(240,240),\n",
        "                                               class_mode='categorical',\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = valied_datagen.flow_from_directory(test_dir,\n",
        "                                                batch_size=32,\n",
        "                                                target_size=(240,240),\n",
        "                                                class_mode='categorical',\n",
        "                                                seed=42)\n",
        "\n",
        "\n",
        "#Create CNN Models\n",
        "model_1 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=10,\n",
        "                        kernel_size=3,\n",
        "                        activation='relu',\n",
        "                        input_shape=(240,240,3)),\n",
        "  tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2,padding='valid'),\n",
        "  tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
        "  tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(3, activation='softmax') #3 class output\n",
        "])\n",
        "\n",
        "#compile Model_1\n",
        "model_1.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_1\n",
        "history_1 = model_1.fit(train_data,\n",
        "                      epochs=5,\n",
        "                      steps_per_epoch=len(train_data),\n",
        "                      validation_data=valid_data,\n",
        "                      validation_steps=len(valid_data))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdCAGKFkaJbe"
      },
      "source": [
        "It's good .!\n",
        "Model_1 did greate job with losses = 0.45 , and accuracy of the model = 0.75\n",
        "\n",
        "The model got ~75% accouracy on the training set , and 77% on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxACFwQKbBuk"
      },
      "source": [
        "## Check Model_1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPOTebnFXR2A"
      },
      "source": [
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5DqC4dib3N8"
      },
      "source": [
        "## Model_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0HF9V8Tb7Sp"
      },
      "source": [
        "run the same model but i will rehape the image size to be (224,224,3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnXdKsXrbHpI"
      },
      "source": [
        "# Set the seed \n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224,224),\n",
        "                                               class_mode='categorical',\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = valied_datagen.flow_from_directory(test_dir,\n",
        "                                                batch_size=32,\n",
        "                                                target_size=(224,224),\n",
        "                                                class_mode='categorical',\n",
        "                                                seed=42)\n",
        "\n",
        "#Create the model with tensorflow playground model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(224,224,3)),\n",
        "    tf.keras.layers.Dense(4, activation='relu'),\n",
        "    tf.keras.layers.Dense(4,activation='relu'),\n",
        "    tf.keras.layers.Dense(3,activation='softmax')\n",
        "])\n",
        "\n",
        "#Compile Model_2\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_2\n",
        "model_2.fit(train_data,\n",
        "            epochs=5,\n",
        "            steps_per_epoch=len(train_data),\n",
        "            validation_data=valid_data,\n",
        "            validation_steps=len(valid_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLKl5KYCiK_A"
      },
      "source": [
        "Note Work ..!\n",
        "\n",
        "The Image shape change it make the model bad on training with 5 epochs , i will try to increes the epoch to = 20, so that the model can take more chance to learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQZZDbpHijg8"
      },
      "source": [
        "## Model_3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uDKg3QJgWsm"
      },
      "source": [
        "#Set Seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Create the model\n",
        "model_3 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224,224,3)),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(4, activation='relu'),\n",
        "  tf.keras.layers.Dense(3,activation='softmax')\n",
        "])\n",
        "\n",
        "#Compile Model_3\n",
        "model_3.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_3\n",
        "history_3 = model_3.fit(train_data,\n",
        "                        epochs=20,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqUMyzg2kfiO"
      },
      "source": [
        "So , It seems our model not learning anything .!\n",
        "\n",
        "for model_4 i will try to give the model more nuron to train with 4 > 100, and add extra layers "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEpQaeRBkJOi"
      },
      "source": [
        "## Model_4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4KTgXETk9W7"
      },
      "source": [
        "#Set random Seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Greate Model_4\n",
        "model_4 = tf.keras.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224,224,3)),\n",
        "  tf.keras.layers.Dense(100 , activation='relu'),\n",
        "  tf.keras.layers.Dense(100 , activation='relu'),\n",
        "  tf.keras.layers.Dense(100 ,activation='relu'),\n",
        "  tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#Compile Model_4\n",
        "model_4.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_4\n",
        "history_4 = model_4.fit(train_data,\n",
        "                        epochs=20,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O8jJzqvmteY"
      },
      "source": [
        "Good job Model_4 , It's seems model_4 get grate job and learn some pattren in the data , it give 72% accouracy in training data , 72% accouracy in test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fw1DXB9oVps"
      },
      "source": [
        "So , To tune the model litteil more , the 72% accourcy it's seems not vary good to me :(\n",
        "\n",
        "\n",
        "**BUT**\n",
        "Start from `model_5` , i will get back to Conv2d Layers , as i make `model_1` as basline model\n",
        "\n",
        "and i will start tune the model from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv5_o_zGpKEZ"
      },
      "source": [
        "## Model_5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSbZnXKqpqug"
      },
      "source": [
        "I will start same `model_1` with more epochs ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az2lyspZqJKy"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhoLyhLXpNxD"
      },
      "source": [
        "#Set random seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Create the model\n",
        "model_5= Sequential([\n",
        "  tf.keras.layers.Conv2D(\n",
        "      filters=10,\n",
        "      kernel_size=3,\n",
        "      activation='relu',\n",
        "      input_shape=(224,224,3)),\n",
        "  tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2,padding='valid'),\n",
        "  tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
        "  tf.keras.layers.Conv2D(10,3,activation='relu'),\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(3,activation='softmax')\n",
        "])\n",
        "\n",
        "#Compile Model_5\n",
        "model_5.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_5\n",
        "history_5 = model_5.fit(train_data,\n",
        "                      epochs=5,\n",
        "                      steps_per_epoch=len(train_data),\n",
        "                      validation_data=valid_data,\n",
        "                      validation_steps=len(valid_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S38UcRkmuMl5"
      },
      "source": [
        "## Model_6 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dblDH8Z4uQzE"
      },
      "source": [
        "Start from `model_5` as basline i will :\n",
        "\n",
        "- Adjast the code \n",
        "- But the augamented data witch i create in strter of this notebook\n",
        "- add more neuron "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV4R74W4v5z4"
      },
      "source": [
        "#Create Augmented data\n",
        "train_data_aug = ImageDataGenerator( rotation_range=30 , #rotate the image by 30%\n",
        "                             width_shift_range=0.1, #shift the img width by max 10%\n",
        "                             height_shift_range=0.1, #shift the img heigh bay max 10%\n",
        "                             rescale=1/255, #rescale img by normalize it\n",
        "                             shear_range=0.2, #cutting away part of the img as buffer \n",
        "                             zoom_range=0.2, #zoom in be 20%\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzlmA7ihwQJ1"
      },
      "source": [
        "# Import data and augment it from training directory\n",
        "print(\"Augmented training images:\")\n",
        "train_data_augmented = train_data_aug.flow_from_directory(train_dir,\n",
        "                                                                   target_size=(224, 224),\n",
        "                                                                   batch_size=32,\n",
        "                                                                   class_mode='categorical',\n",
        "                                                                   shuffle=False) # Don't shuffle for demonstration purposes, usually a good thing to shuffle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAm2ERrhwt_-"
      },
      "source": [
        "# Get data batch samples\n",
        "images, labels = train_data.next()\n",
        "augmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren't augmented, they stay the same"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07JX5n9Px1VN"
      },
      "source": [
        "# Create ImageDataGenerator training instance without data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.) \n",
        "\n",
        "# Create ImageDataGenerator test instance without data augmentation\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc_Gdv5kx7CB"
      },
      "source": [
        "# Create non-augmented data batches\n",
        "print(\"Non-augmented training images:\")\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               target_size=(224, 224),\n",
        "                                               batch_size=32,\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True) # Don't shuffle for demonstration purposes\n",
        "\n",
        "print(\"Unchanged test images:\")\n",
        "test_data = test_datagen.flow_from_directory(test_dir,\n",
        "                                             target_size=(224, 224),\n",
        "                                             batch_size=32,\n",
        "                                             class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giFxf_uLtrZk"
      },
      "source": [
        "# Create the model (this can be our baseline, a 3 layer Convolutional Neural Network)\n",
        "model_6 = Sequential([\n",
        "  Conv2D(10, 3, activation='relu', input_shape=(224, 224, 3)),\n",
        "  MaxPool2D(pool_size=2), # reduce number of features by half\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10, 3, activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile Model_6 \n",
        "model_6.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_6\n",
        "history_6 = model_6.fit(train_data_augmented,\n",
        "                      epochs=5,\n",
        "                      steps_per_epoch=len(train_data_augmented),\n",
        "                      validation_data=test_data,\n",
        "                      validation_steps=len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VIa4uXpxV3X"
      },
      "source": [
        "model_6.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZ2CPs-y9su"
      },
      "source": [
        "### Plot Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-we1BxKmy7_g"
      },
      "source": [
        "# Plot the validation and training data separately\n",
        "def plot_loss_curves(history):\n",
        "  \"\"\"\n",
        "  Returns separate loss curves for training and validation metrics.\n",
        "  \"\"\" \n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  # Plot loss\n",
        "  plt.plot(epochs, loss, label='training_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  # Plot accuracy\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
        "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M1E0U2kzCJY"
      },
      "source": [
        "plot_loss_curves(history_6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B64nwe3YzZwm"
      },
      "source": [
        "I littel wory about overfitting , Becouse the curv of accouracy , i will try to fix it now "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqBX-46rzliP"
      },
      "source": [
        "## Model_7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op-49OsMz5Dw"
      },
      "source": [
        "Make the train data shuffeld"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y95twFNizGC7"
      },
      "source": [
        "print(\"Augmented training images:\")\n",
        "train_data_shuffld = train_data_aug.flow_from_directory(train_dir,\n",
        "                                                                   target_size=(224, 224),\n",
        "                                                                   batch_size=32,\n",
        "                                                                   class_mode='categorical',\n",
        "                                                                   shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV8zN0tN0OKU"
      },
      "source": [
        "# Set the seed \n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Create Model_7\n",
        "model_7 = Sequential([\n",
        "  Conv2D(10,3,activation='relu', input_shape=(224,224,3)),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(3,activation='softmax')\n",
        "])\n",
        "\n",
        "#Compile Model_7\n",
        "model_7.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy']\n",
        "                )\n",
        "\n",
        "#Fit the model\n",
        "history_7 = model_7.fit(train_data_shuffld,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data_shuffld),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_L4Uy3O2SYj"
      },
      "source": [
        "plot_loss_curves(history_7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1PDGAqg4LZQ"
      },
      "source": [
        "Its Learning good , So I will Add more epoch AKA:More chance to learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zzoYXCv4ZtL"
      },
      "source": [
        "## Model_8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDxHl5rJ26Ib"
      },
      "source": [
        "#Set Seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Create Model_8\n",
        "model_8 = Sequential([\n",
        "  Conv2D(10,3,activation='relu',input_shape=(224,224,3)),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(3,activation='softmax')\n",
        "  ])\n",
        "\n",
        "#Compile Model_8\n",
        "model_8.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_8\n",
        "history_8 = model_8.fit(train_data_shuffld,\n",
        "                        epochs=20,\n",
        "                        steps_per_epoch=len(train_data_shuffld),\n",
        "                        validation_data=test_data,\n",
        "                        validation_steps=len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVzVxb8v6QP5"
      },
      "source": [
        "plot_loss_curves(history_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82wvknC7he-"
      },
      "source": [
        "Here are fairly Good model :)\n",
        "\n",
        "The model got 82% accuracy ,\n",
        "Let me fine tune the neroune or maybe add 1 more layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVW29vVF70Bt"
      },
      "source": [
        "## Model_9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DtW7-3g7PKR"
      },
      "source": [
        "#Set Seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Create Model_9\n",
        "model_9 = Sequential([\n",
        "  Conv2D(10,3,activation='relu',input_shape=(224,224,3)),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Conv2D(10,3,activation='relu'),\n",
        "  MaxPool2D(),\n",
        "  Flatten(),\n",
        "  Dense(3,activation='softmax')\n",
        "  ])\n",
        "\n",
        "#Compile Model_9\n",
        "model_9.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_9\n",
        "history_9 = model_9.fit(train_data_shuffld,epochs=20,steps_per_epoch=len(train_data_shuffld),validation_data=test_data,validation_steps=len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvnFmalE9eB_"
      },
      "source": [
        "plot_loss_curves(history_9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48x2acHZ-eW1"
      },
      "source": [
        "# It's Officaly 90% Accouracy :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNcv6EMw-wJD"
      },
      "source": [
        "Give the Model more epochs to learn , 20 > 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKb9ukQh-65J"
      },
      "source": [
        "## Model_10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9KN8IC_-ctN"
      },
      "source": [
        "#Set Seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#Clone Model_9 -> Model_10\n",
        "model_10 = tf.keras.models.clone_model(model_9)\n",
        "\n",
        "#Compile Model_10\n",
        "model_10.compile(loss='categorical_crossentropy',\n",
        "                 optimizer=Adam(),\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "#Fit Model_10\n",
        "history_10 = model_10.fit(train_data_shuffld,epochs=30,steps_per_epoch=len(train_data_shuffld),validation_data=test_data,validation_steps=len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNOHtGmMBG-6"
      },
      "source": [
        "# It's 93% Accuracy :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIBQZ0Xz_8RK"
      },
      "source": [
        "plot_loss_curves(history_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDp4VHuyC0z3"
      },
      "source": [
        "model_10.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ual2Jc_VBWEX"
      },
      "source": [
        "Even thow , The model_10 is overfitting in test_data.\n",
        "\n",
        "Let's try make predection on new image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKBoQeCnDe0q"
      },
      "source": [
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor\n",
        "  and reshapes it to (img_shape, img_shape, colour_channel).\n",
        "  \"\"\"\n",
        "  # Read in target file (an image)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor & ensure 3 colour channels \n",
        "  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "\n",
        "  # Resize the image (to the same size our model was trained on)\n",
        "  img = tf.image.resize(img, size = [img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K7vH1vJBPiF"
      },
      "source": [
        "#Create function to scale and get the image in shape of the model\n",
        "def pred_and_plot(model, filename, class_names):\n",
        "  \"\"\"\n",
        "  Imports an image located at filename, makes a prediction on it with\n",
        "  a trained model and plots the image with the predicted class as the title.\n",
        "  \"\"\"\n",
        "  # Import the target image and preprocess it\n",
        "  img = load_and_prep_image(filename)\n",
        "\n",
        "  # Make a prediction\n",
        "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
        "\n",
        "  # Get the predicted class\n",
        "  pred_class = class_names[int(tf.round(pred)[0][0])]\n",
        "\n",
        "  # Plot the image and predicted class\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Prediction: {pred_class}\")\n",
        "  plt.axis(False);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcWIK_GNDilw"
      },
      "source": [
        "class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1X2pcsENbK"
      },
      "source": [
        "# Make Predection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZgWDqCLEG8l"
      },
      "source": [
        "# Make a prediction using model_10\n",
        "pred_and_plot(model=model_10, \n",
        "              filename=\"tire0.jpeg\", \n",
        "              class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8FHrJUWEWid"
      },
      "source": [
        "pred_and_plot(model=model_10, \n",
        "              filename=\"tire02.jpeg\", \n",
        "              class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmxL3TziFAHD"
      },
      "source": [
        "pred_and_plot(model=model_10, \n",
        "              filename=\"tire03.jpeg\", \n",
        "              class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZL-sc2_FChs"
      },
      "source": [
        "pred_and_plot(model=model_10, \n",
        "              filename=\"tire04.jpeg\", \n",
        "              class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du8XVMQ7FHXV"
      },
      "source": [
        "pred_and_plot(model=model_10, \n",
        "              filename=\"tire05.jpeg\", \n",
        "              class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmLjIG3LFJie"
      },
      "source": [
        "pred_and_plot(model=model_10, \n",
        "              filename=\"tire1.jpeg\", \n",
        "              class_names=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAgm4OZ2FRM_"
      },
      "source": [
        "#Get the Model in tflite Format Label and Model for flutter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eINefs0F95i"
      },
      "source": [
        "print (train_data_shuffld.class_indices)\n",
        "\n",
        "labels = '\\n'.join(sorted(train_data_shuffld.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "4wEIxIsxoZtP"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_10)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9K4QaSJoeVb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}